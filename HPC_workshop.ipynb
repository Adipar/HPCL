{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0bbFzillJlH",
        "outputId": "a646e472-e4be-4792-89f5-ef8f525b285b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Nov  5 09:34:45 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile device_info.cu\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "int main()\n",
        "{\n",
        "        int deviceCount;\n",
        "        cudaGetDeviceCount(&deviceCount);\n",
        "        if (deviceCount == 0)\n",
        "        {\n",
        "                printf(\"There is no device supporting CUDA\\n\");\n",
        "        }\n",
        "        int dev;\n",
        "        for (dev = 0; dev < deviceCount; ++dev)\n",
        "        {\n",
        "                cudaDeviceProp deviceProp;\n",
        "                cudaGetDeviceProperties(&deviceProp, dev);\n",
        "                if (dev == 0)\n",
        "                {\n",
        "                        if (deviceProp.major < 1)\n",
        "                        {\n",
        "                                printf(\"There is no device supporting CUDA.\\n\");\n",
        "                        }\n",
        "                        else if (deviceCount == 1)\n",
        "                        {\n",
        "                                printf(\"There is 1 device supporting CUDA\\n\");\n",
        "                        }\n",
        "                        else\n",
        "                        {\n",
        "                                printf(\"There are %d devices supporting CUDA\\n\", deviceCount);\n",
        "                        }\n",
        "                }\n",
        "                printf(\"\\nDevice %d: \\\"%s\\\"\\n\", dev, deviceProp.name);\n",
        "                printf(\"  Major revision number:                         %d\\n\", deviceProp.major);\n",
        "                printf(\"  Minor revision number:                         %d\\n\", deviceProp.minor);\n",
        "                printf(\"  Total amount of global memory:                 %ld bytes\\n\", deviceProp.totalGlobalMem);\n",
        "                printf(\"  Total amount of constant memory:               %ld bytes\\n\", deviceProp.totalConstMem);\n",
        "                printf(\"  Total amount of shared memory per block:       %ld bytes\\n\", deviceProp.sharedMemPerBlock);\n",
        "                printf(\"  Total number of registers available per block: %d\\n\", deviceProp.regsPerBlock);\n",
        "                printf(\"  Warp size:                                     %d\\n\", deviceProp.warpSize);\n",
        "                printf(\"  Multiprocessor count:                          %d\\n\",deviceProp.multiProcessorCount );\n",
        "\n",
        "                printf(\"  Maximum number of threads per block:           %d\\n\", deviceProp.maxThreadsPerBlock);\n",
        "                printf(\"  Maximum sizes of each dimension of a block:    %d x %d x %d\\n\", deviceProp.maxThreadsDim[0],deviceProp.maxThreadsDim[1], deviceProp.maxThreadsDim[2]);\n",
        "                printf(\"  Maximum sizes of each dimension of a grid:     %d x %d x %d\\n\", deviceProp.maxGridSize[0], deviceProp.maxGridSize[1],  deviceProp.maxGridSize[2]);\n",
        "                printf(\"  Maximum memory pitch:                          %ld bytes\\n\", deviceProp.memPitch);\n",
        "                printf(\"  Texture alignment:                             %ld bytes\\n\", deviceProp.textureAlignment);\n",
        "                printf(\"  Clock rate:                                    %d kilohertz\\n\", deviceProp.clockRate);\n",
        "        }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXbzykhIlXPc",
        "outputId": "3a394957-5e73-4a4e-ec96-d2522bd6cfaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing device_info.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nvcc device_info.cu -o device_info"
      ],
      "metadata": {
        "id": "5_vBVkO3mb5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZZBLhoRmoXZ",
        "outputId": "3b0c8549-60e0-472d-8b21-c03f57465173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;32mdevice_info\u001b[0m*  device_info.cu  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./device_info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bD8J-zuDmsW8",
        "outputId": "b7501539-0fc4-4a3b-b5a4-d749f7f83efd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There is 1 device supporting CUDA\n",
            "\n",
            "Device 0: \"Tesla T4\"\n",
            "  Major revision number:                         7\n",
            "  Minor revision number:                         5\n",
            "  Total amount of global memory:                 15835660288 bytes\n",
            "  Total amount of constant memory:               65536 bytes\n",
            "  Total amount of shared memory per block:       49152 bytes\n",
            "  Total number of registers available per block: 65536\n",
            "  Warp size:                                     32\n",
            "  Multiprocessor count:                          40\n",
            "  Maximum number of threads per block:           1024\n",
            "  Maximum sizes of each dimension of a block:    1024 x 1024 x 64\n",
            "  Maximum sizes of each dimension of a grid:     2147483647 x 65535 x 65535\n",
            "  Maximum memory pitch:                          2147483647 bytes\n",
            "  Texture alignment:                             512 bytes\n",
            "  Clock rate:                                    1590000 kilohertz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile HW.cu\n",
        "#include <stdio.h>\n",
        "\n",
        "// Kernel function to print \"Hello, World!\" from the GPU\n",
        "__global__ void helloFromGPU()\n",
        "{\n",
        "    printf(\"Hello, World from GPU!\\n\");\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    // Print \"Hello, World!\" from the CPU\n",
        "    printf(\"Hello, World from CPU!\\n\");\n",
        "\n",
        "    // Launch kernel with a single thread to print \"Hello, World!\" from the GPU\n",
        "    helloFromGPU<<<4, 4>>>();\n",
        "\n",
        "    // Synchronize to ensure all printf statements from the GPU are executed\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2wWRpERnLHX",
        "outputId": "8ec56ccb-a990-4c79-9e8f-413ba7c4a74d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing HW.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nvcc HW.cu -o HW"
      ],
      "metadata": {
        "id": "UFhY-Kpwo8pF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./HW"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Qf2SsS8pgEx",
        "outputId": "ae395706-59cc-4f09-fea4-1e846d33099f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, World from CPU!\n",
            "Hello, World from GPU!\n",
            "Hello, World from GPU!\n",
            "Hello, World from GPU!\n",
            "Hello, World from GPU!\n",
            "Hello, World from GPU!\n",
            "Hello, World from GPU!\n",
            "Hello, World from GPU!\n",
            "Hello, World from GPU!\n",
            "Hello, World from GPU!\n",
            "Hello, World from GPU!\n",
            "Hello, World from GPU!\n",
            "Hello, World from GPU!\n",
            "Hello, World from GPU!\n",
            "Hello, World from GPU!\n",
            "Hello, World from GPU!\n",
            "Hello, World from GPU!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile grid_block_check.cu\n",
        "#include <stdio.h>\n",
        "\n",
        "// CUDA runtime\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void hello() {\n",
        "\n",
        "    /*********************************************************************************************/\n",
        "\t\tint Global_Block_ID =blockIdx.z * gridDim.x * gridDim.y + blockIdx.y * gridDim.x + blockIdx.x;\n",
        "\n",
        "    /*********************************************************************************************/\n",
        "    int Threads_Per_Block = blockDim.x * blockDim.y * blockDim.z;\n",
        "\n",
        "    /*********************************************************************************************/\n",
        "    int Global_Thread_ID= Global_Block_ID * Threads_Per_Block +((threadIdx.z * blockDim.x * blockDim.y ) + (threadIdx.y *blockDim.x) + threadIdx.x );\n",
        "\n",
        "    /*********************************************************************************************/\n",
        "\n",
        "    printf(\"Global BID : %d| Global TID = %d |I am thread (%d, %d, %d) of block (%d, %d, %d) in the grid\\n\",\n",
        "           Global_Block_ID,Global_Thread_ID,threadIdx.x, threadIdx.y, threadIdx.z,\n",
        "           blockIdx.x, blockIdx.y, blockIdx.z );\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "void printDims(dim3 gridDim, dim3 blockDim) {\n",
        "    printf(\"Grid Dimensions : {%d, %d, %d} blocks. \\n\",\n",
        "    gridDim.x, gridDim.y, gridDim.z);\n",
        "\n",
        "    printf(\"Block Dimensions : {%d, %d, %d} threads.\\n\",\n",
        "    blockDim.x, blockDim.y, blockDim.z);\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv) {\n",
        "\n",
        "\n",
        "    dim3 gridDim(2,2);     // 2 blocks in x and y direction, z default to 1\n",
        "    dim3 blockDim(2,2);  // 4 threads per block: 2 in x direction, 2 in y\n",
        "\n",
        "\n",
        "    printDims(gridDim, blockDim);\n",
        "\n",
        "    printf(\"From each thread:\\n\");\n",
        "    hello<<<gridDim, blockDim>>>();\n",
        "    cudaDeviceSynchronize();      // need for printfs in kernel to flush\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uq3ZFiP0rnKT",
        "outputId": "7a62f1e0-e43b-441d-b09d-9c4ec2c2eacd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing grid_block_check.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nvcc grid_block_check.cu -o grid_block_check"
      ],
      "metadata": {
        "id": "J-pS9rPWr9FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./grid_block_check"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybb0cDCEsEXU",
        "outputId": "b10b3ad0-0390-4189-96ae-a1b3943c446d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid Dimensions : {2, 2, 1} blocks. \n",
            "Block Dimensions : {2, 2, 1} threads.\n",
            "From each thread:\n",
            "Global BID : 2| Global TID = 8 |I am thread (0, 0, 0) of block (0, 1, 0) in the grid\n",
            "Global BID : 2| Global TID = 9 |I am thread (1, 0, 0) of block (0, 1, 0) in the grid\n",
            "Global BID : 2| Global TID = 10 |I am thread (0, 1, 0) of block (0, 1, 0) in the grid\n",
            "Global BID : 2| Global TID = 11 |I am thread (1, 1, 0) of block (0, 1, 0) in the grid\n",
            "Global BID : 0| Global TID = 0 |I am thread (0, 0, 0) of block (0, 0, 0) in the grid\n",
            "Global BID : 0| Global TID = 1 |I am thread (1, 0, 0) of block (0, 0, 0) in the grid\n",
            "Global BID : 0| Global TID = 2 |I am thread (0, 1, 0) of block (0, 0, 0) in the grid\n",
            "Global BID : 0| Global TID = 3 |I am thread (1, 1, 0) of block (0, 0, 0) in the grid\n",
            "Global BID : 3| Global TID = 12 |I am thread (0, 0, 0) of block (1, 1, 0) in the grid\n",
            "Global BID : 3| Global TID = 13 |I am thread (1, 0, 0) of block (1, 1, 0) in the grid\n",
            "Global BID : 3| Global TID = 14 |I am thread (0, 1, 0) of block (1, 1, 0) in the grid\n",
            "Global BID : 3| Global TID = 15 |I am thread (1, 1, 0) of block (1, 1, 0) in the grid\n",
            "Global BID : 1| Global TID = 4 |I am thread (0, 0, 0) of block (1, 0, 0) in the grid\n",
            "Global BID : 1| Global TID = 5 |I am thread (1, 0, 0) of block (1, 0, 0) in the grid\n",
            "Global BID : 1| Global TID = 6 |I am thread (0, 1, 0) of block (1, 0, 0) in the grid\n",
            "Global BID : 1| Global TID = 7 |I am thread (1, 1, 0) of block (1, 0, 0) in the grid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile sm_id.cu\n",
        "#include <stdio.h>\n",
        "#include <stdint.h>\n",
        "\n",
        "static __device__ __inline__ uint32_t __mysmid(){\n",
        "  uint32_t smid;\n",
        "  asm volatile(\"mov.u32 %0, %%smid;\" : \"=r\"(smid));\n",
        "  return smid;}\n",
        "\n",
        "static __device__ __inline__ uint32_t __mywarpid(){\n",
        "  uint32_t warpid;\n",
        "  asm volatile(\"mov.u32 %0, %%warpid;\" : \"=r\"(warpid));\n",
        "  return warpid;}\n",
        "\n",
        "static __device__ __inline__ uint32_t __mylaneid(){\n",
        "  uint32_t laneid;\n",
        "  asm volatile(\"mov.u32 %0, %%laneid;\" : \"=r\"(laneid));\n",
        "  return laneid;}\n",
        "\n",
        "\n",
        "__global__ void mykernel(){\n",
        "\n",
        "  int idx = threadIdx.x+blockDim.x*blockIdx.x;\n",
        "  printf(\"I am thread %d | my SM ID is %d | my warp ID is %d | and my warp lane is %d\\n\", idx, __mysmid(), __mywarpid(), __mylaneid());\n",
        "}\n",
        "\n",
        "int main(){\n",
        "\n",
        "  mykernel<<<41,2>>>();\n",
        "  cudaDeviceSynchronize();\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UA1kt4hIsJCH",
        "outputId": "fbc2b70f-693e-4413-a9c9-10225142aa91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing sm_id.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nvcc sm_id.cu -o sm_id"
      ],
      "metadata": {
        "id": "r5rwZaV6s_v0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./sm_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apV222BbtG94",
        "outputId": "d6a94d1d-25a7-437f-eaa2-6141b8211348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am thread 34 | my SM ID is 34 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 35 | my SM ID is 34 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 74 | my SM ID is 35 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 75 | my SM ID is 35 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 24 | my SM ID is 24 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 25 | my SM ID is 24 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 64 | my SM ID is 25 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 65 | my SM ID is 25 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 38 | my SM ID is 38 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 39 | my SM ID is 38 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 78 | my SM ID is 39 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 79 | my SM ID is 39 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 14 | my SM ID is 14 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 15 | my SM ID is 14 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 54 | my SM ID is 15 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 55 | my SM ID is 15 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 30 | my SM ID is 30 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 31 | my SM ID is 30 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 70 | my SM ID is 31 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 71 | my SM ID is 31 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 28 | my SM ID is 28 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 29 | my SM ID is 28 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 36 | my SM ID is 36 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 37 | my SM ID is 36 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 76 | my SM ID is 37 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 77 | my SM ID is 37 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 20 | my SM ID is 20 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 21 | my SM ID is 20 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 68 | my SM ID is 29 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 69 | my SM ID is 29 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 4 | my SM ID is 4 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 5 | my SM ID is 4 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 44 | my SM ID is 5 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 45 | my SM ID is 5 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 26 | my SM ID is 26 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 27 | my SM ID is 26 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 32 | my SM ID is 32 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 33 | my SM ID is 32 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 72 | my SM ID is 33 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 73 | my SM ID is 33 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 60 | my SM ID is 21 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 61 | my SM ID is 21 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 0 | my SM ID is 0 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 1 | my SM ID is 0 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 80 | my SM ID is 0 | my warp ID is 1 | and my warp lane is 0\n",
            "I am thread 81 | my SM ID is 0 | my warp ID is 1 | and my warp lane is 1\n",
            "I am thread 18 | my SM ID is 18 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 19 | my SM ID is 18 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 58 | my SM ID is 19 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 59 | my SM ID is 19 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 66 | my SM ID is 27 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 67 | my SM ID is 27 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 22 | my SM ID is 22 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 23 | my SM ID is 22 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 10 | my SM ID is 10 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 11 | my SM ID is 10 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 50 | my SM ID is 11 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 51 | my SM ID is 11 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 16 | my SM ID is 16 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 17 | my SM ID is 16 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 56 | my SM ID is 17 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 57 | my SM ID is 17 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 62 | my SM ID is 23 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 63 | my SM ID is 23 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 8 | my SM ID is 8 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 9 | my SM ID is 8 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 48 | my SM ID is 9 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 49 | my SM ID is 9 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 40 | my SM ID is 1 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 41 | my SM ID is 1 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 12 | my SM ID is 12 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 13 | my SM ID is 12 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 52 | my SM ID is 13 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 53 | my SM ID is 13 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 6 | my SM ID is 6 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 7 | my SM ID is 6 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 46 | my SM ID is 7 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 47 | my SM ID is 7 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 2 | my SM ID is 2 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 3 | my SM ID is 2 | my warp ID is 0 | and my warp lane is 1\n",
            "I am thread 42 | my SM ID is 3 | my warp ID is 0 | and my warp lane is 0\n",
            "I am thread 43 | my SM ID is 3 | my warp ID is 0 | and my warp lane is 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matrix_mul.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define MATRIX_SIZE 25000\n",
        "\n",
        "// CUDA kernel for matrix multiplication using global memory\n",
        "__global__ void matrixMultiply(float *A, float *B, float *C, int width) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < width && col < width) {\n",
        "        float sum = 0.0;\n",
        "        for (int k = 0; k < width; ++k) {\n",
        "            sum += A[row * width + k] * B[k * width + col];\n",
        "        }\n",
        "        C[row * width + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int width = MATRIX_SIZE;\n",
        "    size_t size = width * width * sizeof(float);\n",
        "\n",
        "    // Host matrices and result\n",
        "    float *h_A, *h_B, *h_C;\n",
        "    h_A = (float *)malloc(size);\n",
        "    h_B = (float *)malloc(size);\n",
        "    h_C = (float *)malloc(size);\n",
        "\n",
        "    // Initialize matrices A and B\n",
        "    for (int i = 0; i < width * width; ++i) {\n",
        "        h_A[i] = 1.0; // Replace with your initialization\n",
        "        h_B[i] = 2.0; // Replace with your initialization\n",
        "    }\n",
        "\n",
        "    // Device matrices and result\n",
        "    float *d_A, *d_B, *d_C;\n",
        "    cudaMalloc(&d_A, size);\n",
        "    cudaMalloc(&d_B, size);\n",
        "    cudaMalloc(&d_C, size);\n",
        "\n",
        "    // Copy matrices A and B from host to device\n",
        "    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Define grid and block dimensions\n",
        "    dim3 threadsPerBlock(16, 16);\n",
        "    dim3 blocksPerGrid((width + threadsPerBlock.x - 1) / threadsPerBlock.x, (width + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
        "\n",
        "    // Launch kernel for matrix multiplication\n",
        "    matrixMultiply<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, width);\n",
        "    printf(\"Grid dimensions: (%d, %d, %d)\\n\", blocksPerGrid.x, blocksPerGrid.y, blocksPerGrid.z);\n",
        "    printf(\"Threads dimensions : (%d, %d, %d)\\n\",threadsPerBlock.x,threadsPerBlock.y,threadsPerBlock.z);\n",
        "    // Copy matrix C from device to host\n",
        "    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Verify results (print some elements if needed)\n",
        "    printf(\"Sample result: C[0][0] = %f\\n\", h_C[0]);\n",
        "\n",
        "    // Free memory\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(h_C);\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JV41mdxYtYwV",
        "outputId": "33d33331-f2f3-4db7-8319-cd261897dc16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing matrix_mul.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nvcc matrix_mul.cu -o matrix_mul"
      ],
      "metadata": {
        "id": "u-yatVGCt4HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./matrix_mul"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqvivEPht7Tb",
        "outputId": "8eb3facd-3c62-44cd-eaa3-61027318ef59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid dimensions: (1563, 1563, 1)\n",
            "Threads dimensions : (16, 16, 1)\n",
            "Sample result: C[0][0] = 50000.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matrix_mul_shared.cu\n",
        " #include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define MATRIX_SIZE 25000\n",
        "#define TILE_SIZE 16 // Assuming a square tile size for simplicity\n",
        "\n",
        "// CUDA kernel for matrix multiplication using shared memory\n",
        "__global__ void matrixMultiply(float *A, float *B, float *C, int width) {\n",
        "    // Allocate shared memory for tiles of matrices A and B\n",
        "    __shared__ float tileA[TILE_SIZE][TILE_SIZE];\n",
        "    __shared__ float tileB[TILE_SIZE][TILE_SIZE];\n",
        "\n",
        "    // Calculate global row and column indices\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    float sum = 0.0;\n",
        "\n",
        "    // Iterate over tiles\n",
        "    for (int tileIdx = 0; tileIdx < width / TILE_SIZE; ++tileIdx) {\n",
        "        // Load tiles into shared memory\n",
        "        tileA[threadIdx.y][threadIdx.x] = A[row * width + tileIdx * TILE_SIZE + threadIdx.x];\n",
        "        tileB[threadIdx.y][threadIdx.x] = B[(tileIdx * TILE_SIZE + threadIdx.y) * width + col];\n",
        "\n",
        "        // Synchronize threads to ensure all data is loaded\n",
        "        __syncthreads();\n",
        "\n",
        "        // Compute partial sum for the tile\n",
        "        for (int k = 0; k < TILE_SIZE; ++k) {\n",
        "            sum += tileA[threadIdx.y][k] * tileB[k][threadIdx.x];\n",
        "        }\n",
        "\n",
        "        // Synchronize threads before loading the next tile\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Write result to matrix C\n",
        "    if (row < width && col < width) {\n",
        "        C[row * width + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int width = MATRIX_SIZE;\n",
        "    size_t size = width * width * sizeof(float);\n",
        "\n",
        "    // Host matrices and result\n",
        "    float *h_A, *h_B, *h_C;\n",
        "    h_A = (float *)malloc(size);\n",
        "    h_B = (float *)malloc(size);\n",
        "    h_C = (float *)malloc(size);\n",
        "\n",
        "    // Initialize matrices A and B\n",
        "    for (int i = 0; i < width * width; ++i) {\n",
        "        h_A[i] = 1.0; // Replace with your initialization\n",
        "        h_B[i] = 2.0; // Replace with your initialization\n",
        "    }\n",
        "\n",
        "    // Device matrices and result\n",
        "    float *d_A, *d_B, *d_C;\n",
        "    cudaMalloc(&d_A, size);\n",
        "    cudaMalloc(&d_B, size);\n",
        "    cudaMalloc(&d_C, size);\n",
        "\n",
        "    // Copy matrices A and B from host to device\n",
        "    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Define grid and block dimensions\n",
        "    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n",
        "    dim3 blocksPerGrid((width + threadsPerBlock.x - 1) / threadsPerBlock.x, (width + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
        "\n",
        "    // Launch kernel for matrix multiplication\n",
        "    matrixMultiply<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, width);\n",
        "    printf(\"Grid dimensions: (%d, %d, %d)\\n\", blocksPerGrid.x, blocksPerGrid.y, blocksPerGrid.z);\n",
        "    printf(\"Threads dimensions : (%d, %d, %d)\\n\",threadsPerBlock.x,threadsPerBlock.y,threadsPerBlock.z);\n",
        "    // Copy matrix C from device to host\n",
        "    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Verify results (print some elements if needed)\n",
        "    printf(\"Sample result: C[0][0] = %f\\n\", h_C[0]);\n",
        "\n",
        "    // Free memory\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(h_C);\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqcPeAKTt-cW",
        "outputId": "00dc35de-1ad1-4fbf-f309-42829497e1b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing matrix_mul_shared.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nvcc matrix_mul_shared.cu -o matrix_mul_shared"
      ],
      "metadata": {
        "id": "NTAXCYIHvjHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! time ./matrix_mul_shared"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6Paan56vkwe",
        "outputId": "d110f000-1773-4c7a-c32c-5497d1693429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid dimensions: (1563, 1563, 1)\n",
            "Threads dimensions : (16, 16, 1)\n",
            "Sample result: C[0][0] = 49984.000000\n",
            "\n",
            "real\t0m52.134s\n",
            "user\t0m47.594s\n",
            "sys\t0m4.222s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NiCuRoQLvmX8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}